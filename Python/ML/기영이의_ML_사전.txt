지도 학습 : 입력과 출력 샘플 데이터가 있고 주어진 입력으로부터 출력을 예측하고자 할때 사용

- 분류 : 이진분류(Y/N), 다중분류

- 회귀 : 실수 예측, 연속적인 숫자

  출력 값에 연속성이 있는지 없는지를 보고 분류와 회귀를 구분함


-----------------------------------------------------------------------------------------------------------------------------------


- 일반화 : 모델이 처음 보는 데이터에 대해 정확하게 예측할 수 있으면 이를 훈련 세트에서 테스트 세트로 일반화 되었다고 함
  
   일반화 O -> 새로운 데이터 예측 O

   일반화 X -> 새로운 데이터 예측 X


- 과대적합 : 가진 정보를 모두 사용해서 너무 복잡한 모델을 만드는 것 
	  
	  모델이 훈련 세트의 각 샘플에 너무 가깝게 맞춰져 새로운 데이터에 일반화되기 어려울 때 일어남


- 과소적합 : 모델이 너무 간단하고 데이터의 면면과 다양성을 잡아 내지 못할 때를 말함

   
   모델을 복잡하게 할수록 훈련 데이터에 대해서 더 정확히 예측 가능

   그러나 너무 복잡하면 훈련 세트의 각 데이터 포인트에 너무 민감해서 새로운 데이터 일반화를 못함

   다양한 데이터 포인트가 많을수록 과대적합이 없이 더 복잡한 모델을 만들 수 있음

   단, 같은 데이터 포인트를 중복하거나 매우 비슷한 데이터를 모으는 것은 도움되지 않음

- 특성 공학(feature engineering) : 입력 특성 뿐 아니라 특성끼리 곱하여(= 상호작용이라고도 부름) 의도적으로 확장

-----------------------------------------------------------------------------------------------------------------------------------


지도학습 알고리즘

 .score 메서드 모델 평가시 

 회귀 : R2(결정 계수, 회귀 모델의 예측 적합도를 측정한 것으로 0 ~ 1 사이 값, 음수 가능 예측과 타깃이 상반된 경향일 때) 반환

-----------------------------------------------------------------------------------------------------------------------------------
 

- K-NN(Nearest Neighbors) : 가장 간단한 머신러닝 알고리즘, 훈련 데이터셋을 그냥 저장하는 것이 모델 만드는 것에 전부

  이웃의 수가 적을수록 모델의 복잡도가 올라가고 이웃의 수가 많을수록 모델의 복잡도가 올라감
 
  이웃을 많이 사용하면 훈련 데이터에는 잘 안 맞을 수 있지만 더 안정된 예측을 얻게 됨

  -> 회귀 : 여러 개의 최근접 이웃을 사용할 땐 간의 평균이 예측됨

  -> 중요한 매개변수 두 개(데이터 포인트 사이의 거리를 재는 방법 , 이웃의 수)

  -> 이웃 간의 거리를 계산할 때 특성마다 값의 범위가 다르면 범위가 작은 특성에 크게 영향을 받아 정규화 과정이 필요함

  -> 장점 : 적은 데이터에서 매우 빠름, 이해가 쉬움

  -> 단점 : 훈련세트가 클 경우 예측 느림, 전처리 과정 매우 중요, 수백개 이상의 많은 특성 잘 적용X, 특성 값 대부분 0인 데이터X

-----------------------------------------------------------------------------------------------------------------------------------


- 선형 모델(linear model) : 특성이 많은 데이터셋이라면 훌륭한 성능을 낼 수 있음, 
		        모델 파라미터 w와 b를 학습 하는 방법, 모델의 복잡도를 제어하는 방법에서 차이가 남

  -> 선형 회귀(linear regression) = 최소제곱법 
      
      - 예측과 훈련 세트에 있는 타깃 y 사이의 평균제곱오차를 최소화하는 w, b를 찾음
				   
      - mse는 예측값과 타깃 값의 차이를 제곱하여 더한 후 샘플의 개수로 나눔

      - 매개변수가 없는 것이 장점인 동시에 모델의 복잡도를 제어할 방법도 없음

      - 기울기 파라미터 w 는 가중치, 계수라고 하며 Ir 객체의 coef_ 속성에 저장

      - 편향 또는 절편 파라미터 b 는 intercept_ 속성에 저장


   -> 리지 회귀 

      - 회귀를 위한 선형 모델로 가중치의 절댓값을 가능한 작게 만드는 것

      - w의 모든 원소가 0에 가깝게 되길 원함, 직관적으로 생각하면 이는 모든 특성이 출력에 주는 영향을 최소한으로 만듬(기울기)

      - 이런 제약을 '규제' 라고 하며 과대적합이 되지 않도록 모델을 강제로 제한한다는 의미

      - 리지 회귀에 사용하는 규제 방식은 L2 규제라고 함

      - alpha 값의 기본값은 1.0 이며 값을 높이면 0에 더 가깝게 만들어 훈련 세트에 성능은 나빠지지만 일반화에 도움

      - alpha 값을 0.00001로 지정하면 선형 회귀와 같은 score를 보임

      ※ 데이터셋 크기가 400미만에서는 선형 회귀는 어떤 것도 학습을 못하고 있음
          
         두 모델의 성능은 데이터가 많아질수록 좋아지고 마지막엔 선형 회귀가 리지 회귀를 따라잡음

         따라서 데이터가 많으면 리지 회귀와 선형 회귀의 성능이 같아질 것이고 모델이 데이터를 기억하거나 과대적합이 어려워짐

   -> 라소 회귀
       
      - 리지 회귀에서와 같이 라소도 계수를 0에 가깝게 만들려고함, 하지만 방식이 조금 다르고 L1 규제라고함

      - L1 규제의 결과로 라소를 사용할 때 어떤 계수는 정말 0이 됨 = 모델에서 완전히 제외되는 특성이 생김

      - alpha 값과 max_iter(반복 실행하는 최대 횟수)를 활용해 조절

      - 라소 보다는 리지 회귀 모델을 선호 하며 특성이 많고 그 중 일부분만 필요하다면 라소가 좋은 선택일 수 있음

      ※ 라소와 리지의 패널치를 결합한 ElasticNet을 제공, 최상의 성능을 내지만 L1, L2규제를 위한 매개변수 두개를 조정해야 함
       

-----------------------------------------------------------------------------------------------------------------------------------

- 결정 트리 : 분류와 회귀 문제에 널리 사용하는 모델, 기본적으로 결정트리는 결정에 다다르기 위해 예/아니오 질문으로 학습
	   
   -> 결정 트리를 학습한다는 것은 정답에 가장 빨리 도달하는 예/아니오 질문 목록을 학습한다는 뜻   

   -> 머신 러닝에서 이런 질문 들을 테스트라고 함 

   -> 트리 노드 : 질문이나 정답을 담은 네모 상자(마지막 노드는 리프라고도 함)

   -> 엣지 : 질문의 답과 다음 질문을 연결 

   - 1. 트리를 만들 때 알고리즘은 가능한 모든 테스트에서 타깃 값에 대해 가장 많은 정보를 가진 것을 고름

   - 2. 즉, 타깃 값을 가장 잘 나누는 직선을 그리며 루트 노드(전체 데이터 포함)에서 x[1] < 0.06의 테스트에서 분기가 일어남 

   - 3. 테스트를 통과하면 왼쪽 노드에 할당, 못하면 오른쪽 노드에 할당, 가장 좋은 테스트를 찾는 과정을 반복해 모델 생성

   - 4-1. 반복된 프로세스는 각 노드가 테스트 하나씩을 가진 이진 결정 트리를 만듦

   - 4-2. 하나의 축을 따라 데이터를 둘로 나누는 것으로 생각해도 됨
  
   ※ 계층적 영역 분할 알고리즘

   - 5. 각 테스트는 하나의 특성에 대해서만 이루어지므로 나누어진 영역은 항상 축에 평행

   - 6-1. 데이터를 분할하는 것은 각 분할된 영역이(결정 트리의 리프) 

   - 6-2. 한 개의 타깃 값(하나의 클래스나 하나의 회귀분석)을 가질 때 까지 반복 

   - 7. 순수 노드(pure node) : 타깃 하나로만 이뤄진 노드를 이야기 함

   - 8. 새로운 데이터 포인트에 대한 예측은 주어진 데이터 포인트가 특성을 분할한 영역들 중 어디에 놓이는 지 확인하면 됨

   - 9. 영역의 타깃 값 중 다수(순수 노드라면 하나)인 것을 예측 결과로 함

   ※ 회귀의 경우 각 노드의 테스트 결과에 따라 트리를 탐색해나가고 새로운 데이터 포인트에 해당되는 리프노드를 찾음

      찾은 리프노드의 훈련 데이터 평균 값이 이 데이터 포인트의 출력이 됨

   
   -> 결정 트리 복잡도 제어하기  	  









  