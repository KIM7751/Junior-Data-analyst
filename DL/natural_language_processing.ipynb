{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서플로우\n",
    "- 구글이 2015년에 공개한 머신 러닝 오픈소스 라이브러리, 머신 러닝과 딥 러닝을 직관적이고 손쉽게 할 수 있도록 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://ftp.daumkakao.com/pypi/simple\n",
      "Requirement already satisfied: tensorflow in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (2.0.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.6.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.39.0)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: h5py in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (57.4.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.5)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.6.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 케라스(Keras)\n",
    "- 딥 러닝 프레임워크인 텐서플로우에 대한 추상화 된 API를 제공,\n",
    "- 케라스는 백엔드로 텐서플로우를 사용하며, 좀 더 쉽게 딥 러닝을 사용할 수 있게 해줌,텐서플로우 코드를 훨씬 간단하게 작성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://ftp.daumkakao.com/pypi/simple\n",
      "Requirement already satisfied: keras in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (2.3.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: h5py in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.19.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.16.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK와 NLTK Data 설치\n",
    "- 엔엘티케이(NLTK)는 자연어 처리를 위한 파이썬 패키지\n",
    "- NLTK의 기능을 제대로 사용하기 위해서는 NLTK Data라는 여러 데이터를 추가적으로 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoNLPy 설치\n",
    "- 코엔엘파이(KoNLPy)는 한국어 자연어 처리를 위한 형태소 분석기 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://ftp.daumkakao.com/pypi/simple\n",
      "Collecting konlpy\n",
      "  Downloading http://mirror.kakao.com/pypi/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.4 MB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4==4.6.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from konlpy) (4.6.0)\n",
      "Requirement already satisfied: colorama in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from konlpy) (0.3.9)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from konlpy) (4.2.1)\n",
      "Requirement already satisfied: numpy>=1.6 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from konlpy) (1.19.5)\n",
      "Collecting tweepy>=3.7.0\n",
      "  Downloading http://mirror.kakao.com/pypi/packages/67/c3/6bed87f3b1e5ed2f34bd58bf7978e308c86e255193916be76e5a5ce5dfca/tweepy-3.10.0-py2.py3-none-any.whl (30 kB)\n",
      "Collecting JPype1>=0.7.0\n",
      "  Downloading http://mirror.kakao.com/pypi/packages/32/90/a725be43dcaf6f7a20e82a4b672804c9f1139afee3dfd0bc05f2fbb9216f/JPype1-1.3.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
      "\u001b[K     |████████████████████████████████| 448 kB 44.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from JPype1>=0.7.0->konlpy) (3.10.0.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tweepy>=3.7.0->konlpy) (1.16.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from tweepy>=3.7.0->konlpy) (2.25.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.26.6)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.6.8)\n",
      "Installing collected packages: tweepy, JPype1, konlpy\n",
      "Successfully installed JPype1-1.3.0 konlpy-0.5.2 tweepy-3.10.0\n"
     ]
    }
   ],
   "source": [
    "! pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.2'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "konlpy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 판다스 프로파일링(Pandas-Profiling)\n",
    "- 좋은 요리를 위해서는 조리 방법도 중요하지만, 그만큼 중요한 것은 갖고있는 재료의 상태\n",
    "- 좋은 머신 러닝 결과를 얻기 위해서는 데이터의 성격을 파악하는 과정이 선행\n",
    "- 데이터 내 값의 분포, 변수 간의 관계, Null 값과 같은 결측값(missing values) 존재 유무 등을 파악\n",
    "- 데이터를 파악하는 과정을 EDA(Exploratory Data Analysis, 탐색적 데이터 분석) \n",
    "- 방대한 양의 데이터를 가진 데이터프레임을 .profile_report()라는 단 한 줄의 명령으로 탐색하는 패키지인 판다스 프로파일링(pandas-profiling)을 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://ftp.daumkakao.com/pypi/simple\n",
      "Requirement already satisfied: pandas-profiling in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas-profiling) (1.19.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas-profiling) (1.4.1)\n",
      "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas-profiling) (1.1.5)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas-profiling) (4.56.0)\n",
      "Requirement already satisfied: PyYAML>=5.0.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas-profiling) (5.4.1)\n",
      "Requirement already satisfied: htmlmin>=0.1.12 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas-profiling) (0.1.12)\n",
      "Requirement already satisfied: requests>=2.24.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas-profiling) (2.25.1)\n",
      "Requirement already satisfied: visions[type_image_path]==0.7.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas-profiling) (0.7.1)\n",
      "Requirement already satisfied: matplotlib>=3.2.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas-profiling) (3.3.4)\n",
      "Requirement already satisfied: seaborn>=0.10.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas-profiling) (0.11.1)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas-profiling) (1.0.1)\n",
      "Requirement already satisfied: missingno>=0.4.2 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas-profiling) (0.4.2)\n",
      "Requirement already satisfied: pydantic>=1.8.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas-profiling) (1.8.2)\n",
      "Requirement already satisfied: jinja2>=2.11.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas-profiling) (3.0.1)\n",
      "Requirement already satisfied: tangled-up-in-unicode==0.1.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas-profiling) (0.1.0)\n",
      "Requirement already satisfied: phik>=0.11.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas-profiling) (0.11.2)\n",
      "Requirement already satisfied: multimethod==1.4 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling) (1.4)\n",
      "Requirement already satisfied: bottleneck in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling) (1.3.2)\n",
      "Requirement already satisfied: networkx>=2.4 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling) (2.5.1)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling) (21.2.0)\n",
      "Requirement already satisfied: Pillow in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling) (8.2.0)\n",
      "Requirement already satisfied: imagehash in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling) (4.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from jinja2>=2.11.1->pandas-profiling) (2.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib>=3.2.0->pandas-profiling) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib>=3.2.0->pandas-profiling) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib>=3.2.0->pandas-profiling) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib>=3.2.0->pandas-profiling) (2.8.1)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from cycler>=0.10->matplotlib>=3.2.0->pandas-profiling) (1.16.0)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from networkx>=2.4->visions[type_image_path]==0.7.1->pandas-profiling) (4.4.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2021.1)\n",
      "Collecting scipy>=1.4.1\n",
      "  Downloading http://mirror.kakao.com/pypi/packages/c8/89/63171228d5ced148f5ced50305c89e8576ffc695a90b58fe5bb602b910c2/scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9 MB 18.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dataclasses>=0.6 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pydantic>=1.8.1->pandas-profiling) (0.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from pydantic>=1.8.1->pandas-profiling) (3.10.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.24.0->pandas-profiling) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.24.0->pandas-profiling) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.24.0->pandas-profiling) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.24.0->pandas-profiling) (1.26.6)\n",
      "Requirement already satisfied: PyWavelets in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from imagehash->visions[type_image_path]==0.7.1->pandas-profiling) (1.1.1)\n",
      "Installing collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "eqtransformer 0.1.59 requires scipy==1.4.1, but you have scipy 1.5.4 which is incompatible.\n",
      "eqtransformer 0.1.59 requires tqdm==4.48.0, but you have tqdm 4.56.0 which is incompatible.\u001b[0m\n",
      "Successfully installed scipy-1.5.4\n"
     ]
    }
   ],
   "source": [
    "! pip install -U pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./train.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_num</th>\n",
       "      <th>intent</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5087</td>\n",
       "      <td>AS_날짜_요청</td>\n",
       "      <td>제 핸드폰 무상 리퍼 기간 남았는지 확인 좀 해주세요.</td>\n",
       "      <td>네, 무상 리퍼 남았는지 확인 도와드리겠습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9373</td>\n",
       "      <td>AS_날짜_요청</td>\n",
       "      <td>오늘 휴대폰 수리 맡기면 내일 받을 수 있나요?</td>\n",
       "      <td>네, 휴대폰 내일 받으실 수 있습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9374</td>\n",
       "      <td>AS_날짜_요청</td>\n",
       "      <td>오늘 휴대폰 수리 맡기면 이번 주 안으로 받을 수 있나요?</td>\n",
       "      <td>네, 이번주 안으로 받으실 수 있습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9378</td>\n",
       "      <td>AS_날짜_요청</td>\n",
       "      <td>오늘 휴대폰 수리 맡기면 내일 받을 수 있는거죠?</td>\n",
       "      <td>죄송합니다만, 내일 받아보시는건 어렵습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9379</td>\n",
       "      <td>AS_날짜_요청</td>\n",
       "      <td>오늘 휴대폰 수리 맡기면 내일까지 받을 수 있을까요?</td>\n",
       "      <td>아니요, 내일은 어렵습니다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   conv_num    intent                          question  \\\n",
       "0      5087  AS_날짜_요청    제 핸드폰 무상 리퍼 기간 남았는지 확인 좀 해주세요.   \n",
       "1      9373  AS_날짜_요청        오늘 휴대폰 수리 맡기면 내일 받을 수 있나요?   \n",
       "2      9374  AS_날짜_요청  오늘 휴대폰 수리 맡기면 이번 주 안으로 받을 수 있나요?   \n",
       "3      9378  AS_날짜_요청       오늘 휴대폰 수리 맡기면 내일 받을 수 있는거죠?   \n",
       "4      9379  AS_날짜_요청     오늘 휴대폰 수리 맡기면 내일까지 받을 수 있을까요?   \n",
       "\n",
       "                        answer  \n",
       "0  네, 무상 리퍼 남았는지 확인 도와드리겠습니다.   \n",
       "1        네, 휴대폰 내일 받으실 수 있습니다.  \n",
       "2       네, 이번주 안으로 받으실 수 있습니다.  \n",
       "3     죄송합니다만, 내일 받아보시는건 어렵습니다.  \n",
       "4              아니요, 내일은 어렵습니다.  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = data.profile_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f77da8270c43fdb21c3c21096b7dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Summarize dataset', max=17.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517e0a2799f742deb9f394f8f528fd7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Generate report structure', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b193dcbf92b4e9ba1631f61d1bc27ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Render HTML', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f376ac3724664a57aa3e96892d050c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Export report to file', max=1.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pr.to_file('./pr_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 토큰화(Word Tokenization)\n",
    "- 토큰의 기준을 단어(word)로 하는 경우, 단어 토큰화(word tokenization)\n",
    "- 단어(word)는 단어 단위 외에도 단어구, 의미를 갖는 문자열로도 간주되기도 함\n",
    "- 구두점(punctuation)과 같은 문자는 제외시키는 간단한 단어 토큰화 작업\n",
    "- 구두점이란, 마침표(.), 컴마(,), 물음표(?), 세미콜론(;), 느낌표(!) 등과 같은 기호를 말함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word_tokenize는 Don't를 Do 와 n't로 분리하였으며, 반면 Jone's는 Jone과 's로 분리한 것을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- WordPunctTokenizer는 구두점을 별도로 분류하는 특징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 케라스의 text_to_word_sequence는 기본적으로 모든 알파벳을 소문자로 바꾸면서 마침표나 컴마, 느낌표 등의 구두점을 제거\n",
    "- 하지만 don't 나 jone's 와 같은 경우 아포스트로피는 보존 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 구두점이나 특수 문자를 단순 제외해서는 안됨(ex. AT&T, $45.55, 01/02/06)\n",
    "- 줄임말이나 단어 내에 띄어쓰기가 있는 경우(ex. NY, we're, rock 'n' roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 표준 토크나이징 기법 Treebank Tokenization\n",
    "- 하이푼으로 구성된 단어는 하나로 유지\n",
    "- doesn't와 같이 아포스트로피가 '접어'가 함께하는 단어는 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 토큰화(Sentence Tokenization)\n",
    "\n",
    "- 토큰의 단위가 문장(sentence)일 때, 이 작업은 갖고있는 코퍼스 내에서 문장 단위로 구분하는 작업으로 때로는 문장 분류(sentence segmentation)라고도 부름\n",
    "- 보통 갖고있는 코퍼스가 정제되지 않은 상태라면, 코퍼스는 문장 단위로 구분되어있지 않을 가능성이 높음\n",
    "- 사용하고자 하는 용도에 맞게 하기 위해서는 문장 토큰화가 필요\n",
    "- 직관적으로 생각해봤을 때는 ?나 마침표(.)나 ! 기준으로 문장을 잘라내면 되지 않을까라고 생각할 수 있지만, 꼭 그렇지만은 않음\n",
    "- !나 ?는 문장의 구분을 위한 꽤 명확한 구분자(boundary) 역할을 하지만 마침표는 꼭 그렇지 않기 때문 \n",
    "- 다시 말해, 마침표는 문장의 끝이 아니더라도 등장할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mointain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mointain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국어 문장 토큰화(Korean Sentence Splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://ftp.daumkakao.com/pypi/simple\n",
      "Requirement already satisfied: kss in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (3.1.0.4)\n",
      "Requirement already satisfied: emoji in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from kss) (1.4.2)\n",
      "Looking in indexes: http://ftp.daumkakao.com/pypi/simple\n",
      "Collecting regex\n",
      "  Downloading http://mirror.kakao.com/pypi/packages/15/1e/0cc50efade2f9e946274b23aaf86ba2cbede30ed8ca6321ef6c8dcb6debf/regex-2021.8.28-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (745 kB)\n",
      "\u001b[K     |████████████████████████████████| 745 kB 18.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: regex\n",
      "Successfully installed regex-2021.8.28\n"
     ]
    }
   ],
   "source": [
    "! pip install kss\n",
    "! pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: Initializing Kss...\n"
     ]
    }
   ],
   "source": [
    "import kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농답아니에요. 이제 해보면 알걸요?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농답아니에요.', '이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "source": [
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이진 분류기(Binary Classifier)\n",
    "- 문장 토큰화에서의 예외 사항을 발생시키는 마침표의 처리를 위해서 입력에 따라 두 개의 클래스로 분류하는 이진 분류기(binary classifier)를 사용\n",
    "- 두 개의 클래스는 1. 마침표(.)가 단어의 일부분일 경우. 즉, 마침표가 약어(abbreivation)로 쓰이는 경우 2. 마침표(.)가 정말로 문장의 구분자(boundary)일 경우를 의미할 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국어에서의 토큰화의 어려움\n",
    "\n",
    "- 영어는 New York과 같은 합성어나 he's 와 같이 줄임말에 대한 예외처리만 한다면, 띄어쓰기(whitespace)를 기준으로 하는 띄어쓰기 토큰화를 수행해도 단어 토큰화가 잘 작동합니다. 거의 대부분의 경우에서 단어 단위로 띄어쓰기가 이루어지기 때문에 띄어쓰기 토큰화와 단어 토큰화가 거의 같기 때문입니다.\n",
    "\n",
    "- 한국어의 경우에는 띄어쓰기 단위가 되는 단위를 '어절'이라고 하는데 즉, 어절 토큰화는 한국어 NLP에서 지양\n",
    "\n",
    "- 어절 토큰화와 단어 토큰화가 같지 않기 때문\n",
    "\n",
    "- 그 근본적인 이유는 한국어가 영어와는 다른 형태를 가지는 언어인 교착어라는 점에서 기인 \n",
    "\n",
    "- 교착어란 조사, 어미 등을 붙여서 말을 만드는 언어\n",
    "\n",
    "### 한국어는 교착어\n",
    "\n",
    "- 조사라는 것이 존재, 그라는 단어 하나에도 '그가', '그에게', '그를', '그와', '그는'과 같이 다양한 조사가 '그'라는 글자 뒤에 띄어쓰기 없이 바로 붙음\n",
    "\n",
    "- 자연어 처리를 하다보면 같은 단어임에도 서로 다른 조사가 붙어서 다른 단어로 인식이 되면 자연어 처리가 힘들고 번거로워지는 경우가 많음\n",
    "\n",
    "- 대부분의 한국어 NLP에서 조사는 분리해줄 필요가 있음\n",
    "\n",
    "- 즉, 띄어쓰기 단위가 영어처럼 독립적인 단어라면 띄어쓰기 단위로 토큰화를 하면 되겠지만 \n",
    "\n",
    "- 한국어는 어절이 독립적인 단어로 구성되는 것이 아니라 조사 등의 무언가가 붙어있는 경우가 많아서 이를 전부 분리해줘야 한다는 의미\n",
    "\n",
    "- 한국어 토큰화에서는 형태소(morpheme)란 개념을 반드시 이해해야 함\n",
    "\n",
    "- 형태소(morpheme)란 뜻을 가진 가장 작은 말의 단위\n",
    "\n",
    "- 이 형태소에는 두 가지 형태소가 있는데 자립 형태소와 의존 형태소입니다.\n",
    "\n",
    "-- 자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소(=그 자체로 단어), 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등\n",
    "\n",
    "-- 의존 형태소 : 다른 형태소와 결합하여 사용되는 형태소. 접사, 어미, 조사, 어간를 말함\n",
    "\n",
    "문장 : 에디가 딥러닝책을 읽었다\n",
    "\n",
    "자립 형태소 : 에디, 딥러닝책\n",
    "의존 형태소 : -가, -을, 읽-, -었, -다\n",
    "\n",
    "- 이를 통해 유추할 수 있는 것은 한국어에서 영어에서의 단어 토큰화와 유사한 형태를 얻으려면 어형태소 토큰화를 수행해야\n",
    "\n",
    "### 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않음\n",
    "\n",
    "- 사용하는 한국어 코퍼스가 뉴스 기사와 같이 띄어쓰기를 철저하게 지키려고 노력하는 글이라면 좋겠지만, 많은 경우에 띄어쓰기가 틀렸거나, 지켜지지 않는 코퍼스가 많음\n",
    "\n",
    "- 한국어의 경우 띄어쓰기가 지켜지지 않아도 글을 쉽게 이해할 수 있는 언어\n",
    "\n",
    "EX1) 제가이렇게띄어쓰기를전혀하지않고글을썼다고하더라도글을이해할수있습니다\n",
    "\n",
    "EX2) Tobeornottobethatisthequestion\n",
    "\n",
    "- 반면, 영어의 경우에는 띄어쓰기를 하지 않으면 손쉽게 알아보기 어려운 문장들이 생김\n",
    "\n",
    "- 이는 한국어(모아쓰기 방식)와 영어(풀어쓰기 방식)라는 언어적 특성의 차이에 기인\n",
    "\n",
    "- 결론은 한국어는 수많은 코퍼스에서 띄어쓰기가 무시되는 경우가 많아 자연어 처리가 어려워졌다는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 품사 태깅(Part-of-speech tagging)\n",
    "\n",
    "- 단어는 표기는 같지만, 품사에 따라서 단어의 의미가 달라지기도 함\n",
    "- 단어 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지를 구분해놓기도 하는데, 이 작업을 품사 태깅(part-of-speech tagging)이라고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('actively', 'RB'),\n",
       " ('looking', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('students', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('and', 'CC'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('student', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한국어 자연어 처리를 위해서는 KoNLPy(\"코엔엘파이\")라는 파이썬 패키지를 사용\n",
    "- 코엔엘파이를 통해서 사용할 수 있는 형태소 분석기로 Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import  Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n"
     ]
    }
   ],
   "source": [
    "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "print(okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "print(okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) morphs : 형태소 추출\n",
    "- 2) pos : 품사 태깅(Part-of-speech tagging)\n",
    "- 3) nouns : 명사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import  Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n"
     ]
    }
   ],
   "source": [
    "print(kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n"
     ]
    }
   ],
   "source": [
    "print(kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "print(kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 필요 용도에 어떤 형태소 분석기가 가장 적절한지를 판단하고 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
