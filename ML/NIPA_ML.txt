빅데이터의 등장

- 4차 산업 흐름에 따라 수 많은 정보가 디지털 데이터로 저장되고 있음

- 빅데이터를 통하여 IOT, 클라우드, 머신러닝 기술이 상호 협력함

-----------------------------------------------------------------------------------------------------------------------------------


빅데이터와 머신러닝

- 머신러닝은 빅 데이터를 분석할 수 있는 강력한 툴

- 기존 통계학 및 시각화 방법의 한계를 해결 ex) 예측, 패턴 파악, 추천 시스템

-----------------------------------------------------------------------------------------------------------------------------------


머신러닝

- 명시적으로 프로그래밍을 하지 않고도 컴퓨터가 학습할 수 있는 능력을 갖게 하는 것 ex) Train, Predict, Action

- 기법 구분 : 지도학습 - 회귀분석, 분류 / 비지도학습 / 강화학습 

- 지도학습 vs 비지도 학습 : 예측해야 할 결과에 대한 정답 데이터가 있는지 확인 / 정답이 존재하면 지도 학습

- 강화학습 : 데이터가 없어도 가능, 기계가 스스로 학습할 데이터를 생성, 만든 데이터를 토대로 학습

-----------------------------------------------------------------------------------------------------------------------------------


자료의 형태

- 머신러닝은 데이터라는 디지털 자료를 바탕으로 수행하는 분석방식, 자료의 형태를 파악함은 머신러닝을 사용하기 위한 필수과정

- 데이터가 어떻게 구성되어 있을까? , 어떤 머신러닝 모델을 사용해야 할까? , 데이터 전처리를 어떻게 해야 할까? ,,,

- 수치형 자료 : 연속형 자료 / 이산형 자료 //// 범주형 자료 : 순위형 자료 / 명목형 자료  

- 수치형 자료 = 양적 자료, 수치로 측정이 가능한 자료 ex) 키, 몸무게, 시험 점수, 나이 등

	- 연속형 자료 = 연속적인 관측값을 가짐 ex) 원주율, 시간

	- 이산형 자료 = 셀 수 있는 관측값을 가짐 ex) 뉴스 글자 수, 주문 상품 개수

- 범주형 자료 = 질적 자료, 수치로 측정이 불가능한 자료 ex) 성별, 지역, 혈액형

	- 순위형 자료 = 범주 사이의 순서에 의미가 있음 ex) 학점(A+, A, A-)

	- 명목형 자료 = 범주 사이의 순서에 의미가 없음 ex) 혈액형(A, B, O, AB)

	- 범주형 자료와 수치형 자료의 구분 ≠ 자료의 숫자 표현 가능 여부
	  
	1) 범주형 자료가 숫자로 표현되는 경우 ex) 남녀 성별 구분시 남자를 1, 여자를 0으로 숫자로 표현했으나 범주형 자료

	2) 수치형 자료를 범주형 자료로 변환하는 경우 ex) 나이 구분 시 값은 수치지만 10~19세 등 구간화하면 범주형 자료




1) 범주형 자료의 요약

- 다수의 범주가 반복해서 관측, 관측값의 크기보다 포함되는 범주에 관심  → 범주형 자료 요약 필요 

- 각 범주간에 크기 차이와 그것에 따른 유의미한 결과를 알아내기 위함

- 방법 : 1) 각 범주에 속하는 관측값의 개수를 측정

          2) 전체에서 차지하는 각 범주의 비율 파악

          3) 효율적으로 범주 간의 차이점을 비교 가능

- 도수분포표 : 도수(관측값의 개수), 상대도수(전체에서 차지하는 범주의 비율 파악), 누적상대도수(총합 1.0)

	- 도수 : value_counts( ), 상대도수 : value_counts(normalize = True)

	※ value_counts ( ) 사용은 Series로 사용해야함

- 막대 그래프 : 각 범주 들의 크기 차이를 시각적으로 잘 보여줌, 각 범주에서 도수의 크기를 막대로 그림

	- 장점 : 각 범주가 가지는 도수의 크기 차이를 비교하기 쉬움

	- 단점 : 각 범주가 차지하는 비율의 비교는 어려움

	- plt.bar( x, y ) / x = 라벨 , y = 도수




2) 수치형 자료의 요약

- 범주형 자료와 달리 수치로 구성되어 있기에 통계값을 사용한 요약이 가능함

- 시각적 자료로는 이론적 근거 제시가 쉽지 않은 단점을 보완 

→ 많은 양의 자료를 의미 있는 수치로 요약하여 대략적인 분포상태를 파악 가능 

- 평균(mean) : np.mean( ), 관측값들을 대표할 수 있는 통계값, 수치형 자료의 통계값 중 가장 많이 사용

	- 장점 : 관측값의 산술 평균을 사용해 기초적인 통계 수치로써 많이 사용

	- 단점 : 극단적으로 큰 값이나 작은 값의 영향을 많이 받음

- 분산 : from statistics import variance variance( ), 자료가 얼마나 흩어졌는지 숫자로 표현, 관측값의 평균으로 부터 떨어진 정도

	- 분산이 작다 : 관측값이 평균 근처에 가깝다

	- 분산이 크다 : 관측값이 평균으로부터 골고루 퍼져있다

- 표준편차 : from statistics import stdev stdev( ), 분산의 양의 제곱근

	- 분산의 단위 = 관측값의 단위의 제곱 → 관측값의 단위와 불일치

	- 분산의 양의 제곱근은 관측값과 단위가 일치 → 분산의 양의 제곱근을 표춘편차라고 하고 s로 표기 

- 히스토그램(histogram) : 수치형 자료를 일정한 범위를 갖는 범주로 나누고 막대 그래프와 같은 방식으로 그림

	- plt.hist( ) // x = 계급 , y = 빈도

	- 자료의 분포를 알 수 있음, 계급구간과 막대의 높이로 그림, 도수 & 상대도수를 막대 높이로 사용

	- 도수 비교 : 범주형 → 막대그래프 / 수치형 → 히스토그램

-----------------------------------------------------------------------------------------------------------------------------------


데이터 전처리

- 과정 : 데이터 수집(크롤링, DB데이터) -> 데이터 분석 및 전처리(머신러닝에 사용할 형태로) -> 머신러닝 학습(모델 이용)
	-> 머신러닝 평가(평가용 데이터를 사용해 평가)

- 역할 : 데이터 변환, 데이터 정제, 데이터 분리

	- 데이터 변환 : 대부분의 머신러닝 모델은 숫자 데이터를 입력 받음, 실제 데이터는 모델이 이해할 수 없는 형태임
		      이해할 수 있게끔 해주는 과정이 "전처리" 라고 볼 수 있음

	- 데이터 정제 : 결측값 및 이상치를 처리

	- 데이터 분리 : 학습용과 평가용 데이터를 분리, 전체를 학습시키면 평가시 객관성이 떨어짐



1) 데이터 변환

(1) 범주형 자료 전처리

- 범주형 데이터는 몇 개의 범주로 나누어진 자료

- 범주의 크기가 의미 없다면 명목형 자료, 크기가 의미 있다면 순서형 자료

	- 명목형 자료 : 수치 맵핑 방식(일반적으로 0과 1로 맵핑, -1과 1 0과 100 다양 케이스 존재해 모델별로 차이가 남), 
		      더미(변수를 더 만들어 0과 1로 맵핑) 기법

		      DataFrame.replace({A:B, C:D,...})

		      pd.get_dummies(DataFrame[[변수명]])

	- 순서형 자료 : 수치 맵핑 방식(수치에 맵핑하여 변환 하지만, 수치간 크기의 차이는 커스텀 가능, 크기 차이 모델 영향)




(2) 수치형 자료 전처리

- 크기를 갖는 수치형 값으로 이루어진 데이터

- 머신러닝의 입력으로 바로 사용할 수 있으나, 모델 성능을 높이기 위해 데이터 변환이 필요

	- 스케일링 : 변수 값의 범위 및 크기를 변환하는 방식, 변수 간의 범위 차이가 나면 사용

		- 정규화 : 변수 X를 정규화한 값 X' 
		
				
			X' =  X - X(min) / X(max) - X(min)

			가격(억단위), 평수(천단위)의 변수를 비교했을 때 가격 변수의 자릿수가 훨씬 커
			가격에 대한 영향을 더 많이 받는다고 볼 수 있음 따라서 비슷한 선상에서 비교할 수 있게
			같은 스케일로 맞춰주는 작업이라고 보면 됨 0 ~100 -> 0 ~ 1 

			DataFrame에서 min(), max() 메서드를 사용하여 변환을 수행

			def normal(data):
    
    				data = (data - min(data)) / (max(data) - min(data))
     
    				return data



		- 표준화 : X' = X - 평균 / 표준편차

			평균이 0이고 표준편차가 1인 표준정규분포로 바꿔줌 -2 ~ 0 ~ 2

			DataFrame에서 mean(), std() 메서드를 사용하여 변환

			def standard(data):
    
    				data = (data - data.mean()) / data.std()
    
    				return data

	
	
	- 범주화 : 변수 값보다 범주가 중요한 경우 사용

		점수 예측이 아닌 평균 점수 보다 높은지 낮은지 예측하고 싶을때 쓸 수 있음
		평균 54.63 // 평균 이상 1 // 평균 이하 0



2) 데이터 정제 및 분리

(1) 결측치 처리하기

- 일반적인 머신러닝 모델의 입력 값으로 결측값을 사용할 수 없음

- 따라서 Null, None, NaN 등의 결측값을 처리 해야함

	- 샘플 삭제 : 결측값이 존재하는 샘플 삭제

	
	- 변수 삭제 : 결측값이 많이 존재하는 변수 삭제 

	
	- 다른 값으로 대체 : 결측값을 다른 값으로 대체(평균, 중앙값, 머신러닝 예측)

(2) 이상치 처리하기

- 모델의 성능을 저하할 수 있음

- 이상치는 일반적으로 전처리 과정에서 제거

- 어떤 값이 이상치 인지 판단하는 기준이 중요

	- 통계 지표(카이제곱 검정, IQR 지표 등)를 사용하여 판단

	- 데이터 분포를 보고 직접 판단

	- 머신러닝 기법을 사용하여 이상치 분류


(3) 데이터 분리 

- 7 : 3, ~ 8 : 2 비율로 학습용 평가용 데이터로 나눔

- 지도학습의 경우 feature 데이터와 label 데이터를 분리하여 저장

	- feature 데이터 : lable을 예측하기 위한 입력 값

	- label 데이터 : 예측해야 할 대상이 되는 데이터

-----------------------------------------------------------------------------------------------------------------------------------

회귀

- 일관성 있는 경향을 보이는 관계를 "선형 관계"라고 함 // 비례, 반비례 관계를 예로 들 수 있음

- 회귀 분석 : 데이터를 가장 잘 설명하는 모델을 찾아 입력값에 따른 미래 결과값(수치형)을 예측하는 알고리즘 

	X : 평균기온, Y : 아이스크림 판매량  - 기존 기온에 따른 판매량을 2차원 실좌표공간에 점을 찍어보고 경향을 따라가는 직선을 그어 가정을 하면서 접근
		                  	      - 직선을 수식으로 나타내면 다음과 같음
		 	                   - 기존에 없던 평균 기온을 기록했을 때 판매량을 잘 예측할 수 있는 직선의 계수 찾는게 관건

	가정 : y = β0 + β1X -> 적절한 β0, β1을 찾자(= 데이터를 잘 설명하는 모델) 

- 완벽한 예측은 불가능하기에 최대한 잘 근사해야 함

- 각 데이터의 실제 값과 모델이 예측하는 값의 차이를 최소한으로 하는 선을 찾자

- 단순 선형 회귀 모델을 학습하며 차이를 최소한으로 하는 선을 찾아 보자(β0, β1)

-----------------------------------------------------------------------------------------------------------------------------------

단순 선형 회귀

- 데이터를 설명하는 모델을 직선 형태로 가정 y = β0(절편) + β1X(기울기), 두 값을 구하는 것이 우리의 목표 

	beta_0 = lrmodel.intercept_ 절편 
   	beta_1 = lrmodel.coef_[0] 기울기

- 실제 정답과 내가 예측한 값과의 차이가 적은 것이 좋음(= 데이터를 잘 설명함)

- 실제 값과 예측 값의 차이를 구해봄 (실제값 - 예측값)

	차이의 합으로 비교하기에는 예외가 있음 (ex. 1 + 0 + -1 = 0 vs 0 + 0 + 0 = 0)  

	차이의 제곱의 합으로 비교, 차이의 제곱합을 손실 함수(Loss Function)로 정의
	
	손실 함수가 작을수록 좋은 모델이다

- 손실 함수에서 주어진 값은 실제 값이다 = β0(절편), β1(기울기) 값을 조절 하여 손실 함수의 크기를 작게 한다 

- 대표적인 손실 함수에는 평균 제곱 오차(MSE), 교차 엔트로피 오차(Cross Entropy Error)가 있음

- 손실 함수의 크기를 작게 하는 β0(절편), β1(기울기)를 찾는 대표적인 방법으로 경사 하강법(Gradient descent)이 존재

-----------------------------------------------------------------------------------------------------------------------------------

경사 하강법 

- 손실 함수 값을 제일 작게 하는 절편, 기울기를 β0*, β1* 라고 하자 

- 경사 하강법은 계산 한번으로 β0*, β1*을 구하는 것이 아니라 초기값에서 점진적으로 구하는 방식

	0, 1 -> 1, 3 ->3, 7

- β0, β1 값을 손실 함수 값이 작아지게 계속 업데이트 하는 방법


	1) β0, β1 값을 랜덤하게 초기화 		ex) 0 1

	2) 현재 β0, β1 값으로 손실 값(MSE) 계산	ex) 100	

	3) 현재 β0, β1 값을 어떻게 변화해야 손실 값을 줄일 수 있는지 알 수 있는 gredient값(힌트와 같다고 보면 됨) 계산, β0, β1에 대응하는 값이 나옴 ex) 3 2 

	4) gredient 값을 활용하여 β0, β1 값 업데이트

	5) 손실 값의 차이가 거의 없어질 때까지 2~4번 과정을 반복 (손실 값과 차이가 줄어들면 gredient 값도 작아짐)

- 단순 선형 회귀 특징

	1) 가장 기초적이나 여전히 많이 사용되는 알고리즘

	2) 입력값이 1개인 경우에만 적용이 가능 (독립변수 하나, 단점)

	3) 입력값과 결과값의 관계를 알아보는데 용이함

	4) 입력값이 결과값에 얼마나 영향을 미치는지 알 수 있음

	5) 두 변수 간의 관계를 직관적으로 해석하고자 하는 경우 활용


단순 선형 회귀 모델이 어떻게 생겼는지?, 직선형 모델에 대해서 배움 -> 어떻게 잘 설명할지 손실함수에 대해서 배움 -> 손실함수를 줄이는 방법으로 경사하강법을 알아봄

-----------------------------------------------------------------------------------------------------------------------------------

다중 선형 회귀

- 입력값 X에 강수량이 추가 된다면? 즉, 평균 기온과 평균 강수량에 따른 아이스크림 판매량을 예측하고자 할 때 X1, X2

- 여러 개의 입력값(X)으로 결과값(Y)을 예측하고자 하는 경우, 단순 선형 회귀로는 안됨

- 입력값 X가 여러 개(2개 이상)인 경우 활용할 수 있는 회귀 알고리즘, 각 개별 Xi 에 해당하는 최적의 βi를 찾아야 함

	Y = β0 + β1X1 + β2X2 + β3X3 + ... + BmXm

- 평균 기온과 평균 강수량에 따른 아이스크림 판매량을 예측하고자 함

	Y = β0 + β1X1(평균 기온) + β2X2(평균 강수량)

- 단순 선형 회귀와 마찬가지로 선형 관계를 가정함!!

- 단순 선형 회귀와 마찬가지로 손실 함수(MSE)는 입력값과 실제값 차이의 제곱의 합으로 정의

- 마찬가지로 경사하강법을 이용하여 β 값 업데이트 

- 다중 선형 회귀 특징

	1) 여러 개의 입력값과 결과값 간의 관계 확인 가능

	2) 어떤 입력값이 결과값에 어떠한 영향을 미치는지 알 수 있음 -> β1 β2의 크기를 비교해 Y 값에 얼마나 영향을 미치는지 확인 할 수 있음 

	3) 여러 개의 입력값 사이 간의 "상관 관계"가 높을 경우 결과에 대한 신뢰성을 잃을 가능성이 있음 -> 독립 변수는 종속 변수에 영향을 끼치는데에 있어서 의미가 있음 하지만 독립 변수들끼리 영향을 주면 독립 변수로써 의미가 없어짐 

		i) 상관 관계 : 두 가지 것의 한쪽이 변화하면 다른 한쪽도 따라서 변화하는 관계

-----------------------------------------------------------------------------------------------------------------------------------

분류

- 가정하기 : 해외 여행을 준비 중에 완벽한 여행을 위해 항공 지연을 피하고자 함, 기상 정보(구름 양, 풍속)를 활용하여 해당 항공의 지연 여부를 예측할 수 있다면?

- 문제정의 : 과거 기상 정보(X, 풍속)와 그에 따른 항공 지연 여부(Y)

- 현재 풍속에 따른 항공 지연 여부 예측 

- 예측해야 하는 값이 범주형(클래스)일 때 우리는 분류 알고리즘을 사용함

	1) 주어진 입력 값이 어떤 클래스에 속할지에 대한 결과 값을 도출하는 알고리즘

	2) 다양한 분류 알고리즘이 존재하며 예측 목표와 데이터 유형에 따라 적용

- 종류

	1) 트리 구조 기반 : 의사결정나무, 랜덤포레스트

		i) 의사결정 나무 : 간단하지만 성능이 우수, 이 모델을 이용해 확장할 수 있는 모델이 많음

	2) 확률 모델 기반 : 나이브 베이즈 분류기

	3) 결정 경계 기반 : 선형 분류기, 로지스틱 회귀, SVM

	4) 신경망 : 퍼셉트론, 딥러닝 모델

-----------------------------------------------------------------------------------------------------------------------------------

의사 결정 나무

- 스무고개와 같이 특정 질문들을 통해 정답을 찾아가는 모델 

- 최상단의 뿌리 마디(Root Node)에서 마지막 끝 마디(Terminal Node)까지 아래 방향으로 진행

	ex) 풍속 4m/s 를 기준으로 분리, thresh hold = 풍속

- 중간 마디(Internal Node) 추가 

	ex) 2를 기준으로분리(뿌리 마디) , 4를 기준으로 분리(중간마디)

-----------------------------------------------------------------------------------------------------------------------------------

의사 결정 나무의 불순도

- 데이터의 불순도(impurity)를 최소화하는 구역으로 나누자

	1) 불순도 : 다른 데이터가 섞여 있는 정도

	2) 많을수록 불순도가 높다라고 이야기하며 최소화 하는 방향을 설계하는 것이 의사 결정 나무의 성능을 높임

	X : 1, 2, 3, 4, 5, 6, 7, 8
	Y : N, N, Y, N, N, N, Y, Y 

	-> 뿌디 마디에서 기준을 잡아 나누는데 다른 성격의 데이터가 섞여 있다고 볼 수 있는 3을 확인할 수 있음
	
	-> 이때 3을 기준으로 나누는 것 보다 7을 기준으로 나누는 것이 불순도를 최소화 한다고 볼 수 있음 
  
- 데이터 개수가 적으면 눈으로 확인 가능한데 수많은 데이터가 존재할 때 불순도는 어떻게 측정할 수 있을까?

	1) 지니 불순도(Gini Impurity)

		i) 지니 계수(Gini Index) : 해당 구역 안에서 특정 클래스에 속하는 데이터의 비율을 모두 제외한 값, "다양성을 계산"하는 방법

			1 - (yes의 확률)**2 - (no의 확률)**2

		ii) 지니 불순도(Gini Impurity) :  
			
			n1 / N(Gini1) + n2 / N(Gini2) 

			Gini1, Gini2 : 뿌리 마디에서 분리한 마디 각각의 지니계수를 구한 것을 의미

			ni : i번째 자식 마디의 데이터 개수(나눠진 마디에 데이터 개수), N : 부모 마디의 데이터 개수(데이터 총 개수)

- 마디에서 나누는 기준 thresh hold는 불순도가 낮은 것으로 선택해야 함

-----------------------------------------------------------------------------------------------------------------------------------

의사 결정 나무의 깊이 trade off 

- 중간 마디의 개수

- 의사결정나무의 깊이가 깊어질수록 세분화해서 나눌 수 있음, 너무 깊은 모델은 과적합을 야기할 수 있음

- 데이터에 따라 다를 수 있지만 너무 깊은 모델은 지양 

-----------------------------------------------------------------------------------------------------------------------------------

의사 결정 나무의 특징

- 결과가 직관적이며, 해석하기 쉬움

- 나무 깊이가 깊어질수록 과적합 문제 발생 가능성이 매우 높음

- 학습이 끝난 트리의 작업 속도가 매우 빠름

-----------------------------------------------------------------------------------------------------------------------------------

분류 평가 지표

- 혼동 행렬(Confusion Matrix) : 분류 모델의 성능을 평가 하기 위함

		          예측
		Positive     Negative

           Positive         TP	     FN
     실제   
           Negative       FP            TN

- 정확도(Accuracy) : 전체 데이터 중 제대로 분류된 데이터의 비율로 모델이 얼마나 정확하게 분류하는지를 나타냄

		일반적으로 분류 모델의 주요 평가 방법으로 사용됨

		※ 클래스 비율이 불균형 할 경우 평가 지표의 신뢰성을 잃을 가능성이 있음

		Accuracy = TP + TN / P + N

			P : TP + FN
			N : TN + FP 

- 정밀도(Precision) : 모델이 Positive라고 분류한 데이터 중에서 실제로 Positive인 데이터 비율

		실제로 Negtive인 데이터를 Positive라고 판단하면 안되는 경우 사용되는 지표

		Precision = TP / TP + FP

		ex) 일반 메일을 스팸 메일(positive)로 잘못 예측했을 경우 중요한 메일을 전달받지 못하는 상황이 발생할 수 있음


- 재현율(Recall) : 실제로 Positive인 데이터 중에서 모델이 Positive로 분류한 데이터의 비율

	         Positive인 데이터를 Negative라고 판단하면 안되는 경우 사용되는 지표

	         Recall = TP / TP + FN = TP / P

	         ex) 악성 종양(Positive)을 양성 종양(Negative)으로 잘못 예측했을 경우 제 때 치료를 받지 못하게 되어 생명이 위급해질 수 있음

- 분류 목적에 따라 다양한 지표를 계산하여 평가

	        분류 결과를 전체적으로 보고 싶다 -> 혼동 행렬(Confusion Matrix)
	  
	        정답을 얼마나 잘 맞췄는지 -> 정확도(Accuracy)

	        FP 또는 FN의 중요도가 높다면 -> 정밀도(Precision), 재현율(Recall)




	 


