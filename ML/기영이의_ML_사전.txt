지도 학습 : 입력과 출력 샘플 데이터가 있고 주어진 입력으로부터 출력을 예측하고자 할때 사용

- 분류 : 이진분류(Y/N), 다중분류

- 회귀 : 실수 예측, 연속적인 숫자

  출력 값에 연속성이 있는지 없는지를 보고 분류와 회귀를 구분함


-----------------------------------------------------------------------------------------------------------------------------------


- 일반화 : 모델이 처음 보는 데이터에 대해 정확하게 예측할 수 있으면 이를 훈련 세트에서 테스트 세트로 일반화 되었다고 함
  
   일반화 O -> 새로운 데이터 예측 O

   일반화 X -> 새로운 데이터 예측 X


- 과대적합 : 가진 정보를 모두 사용해서 너무 복잡한 모델을 만드는 것 
	  
	  모델이 훈련 세트의 각 샘플에 너무 가깝게 맞춰져 새로운 데이터에 일반화되기 어려울 때 일어남


- 과소적합 : 모델이 너무 간단하고 데이터의 면면과 다양성을 잡아 내지 못할 때를 말함

   
   모델을 복잡하게 할수록 훈련 데이터에 대해서 더 정확히 예측 가능

   그러나 너무 복잡하면 훈련 세트의 각 데이터 포인트에 너무 민감해서 새로운 데이터 일반화를 못함

   다양한 데이터 포인트가 많을수록 과대적합이 없이 더 복잡한 모델을 만들 수 있음

   단, 같은 데이터 포인트를 중복하거나 매우 비슷한 데이터를 모으는 것은 도움되지 않음

- 특성 공학(feature engineering) : 입력 특성 뿐 아니라 특성끼리 곱하여(= 상호작용이라고도 부름) 의도적으로 확장

-----------------------------------------------------------------------------------------------------------------------------------


지도학습 알고리즘

 .score 메서드 모델 평가시 

 회귀 : R2(결정 계수, 회귀 모델의 예측 적합도를 측정한 것으로 0 ~ 1 사이 값, 음수 가능 예측과 타깃이 상반된 경향일 때) 반환

-----------------------------------------------------------------------------------------------------------------------------------
 

- K-NN(Nearest Neighbors) : 가장 간단한 머신러닝 알고리즘, 훈련 데이터셋을 그냥 저장하는 것이 모델 만드는 것에 전부

  이웃의 수가 적을수록 모델의 복잡도가 올라가고 이웃의 수가 많을수록 모델의 복잡도가 올라감
 
  이웃을 많이 사용하면 훈련 데이터에는 잘 안 맞을 수 있지만 더 안정된 예측을 얻게 됨

  -> 회귀 : 여러 개의 최근접 이웃을 사용할 땐 간의 평균이 예측됨

  -> 이웃 간의 거리를 계산할 때 특성마다 값의 범위가 다르면 범위가 작은 특성에 크게 영향을 받아 정규화 과정이 필요함

  -> 매개 변수 : 데이터 포인트 사이의 거리를 재는 방법 , 이웃의 수

  -> 장점 : 적은 데이터에서 매우 빠름, 이해가 쉬움

  -> 단점 : 훈련세트가 클 경우 예측 느림, 전처리 과정 매우 중요, 수백개 이상의 많은 특성 잘 적용X, 특성 값 대부분 0인 데이터X

-----------------------------------------------------------------------------------------------------------------------------------


- 선형 모델(linear model) : 특성이 많은 데이터셋이라면 훌륭한 성능을 낼 수 있음, 
		        모델 파라미터 w와 b를 학습 하는 방법, 모델의 복잡도를 제어하는 방법에서 차이가 남

  -> 선형 회귀(linear regression) = 최소제곱법 
      
      - 예측과 훈련 세트에 있는 타깃 y 사이의 평균제곱오차를 최소화하는 w, b를 찾음
				   
      - mse는 예측값과 타깃 값의 차이를 제곱하여 더한 후 샘플의 개수로 나눔

      - 매개변수가 없는 것이 장점인 동시에 모델의 복잡도를 제어할 방법도 없음

      - 기울기 파라미터 w 는 가중치, 계수라고 하며 Ir 객체의 coef_ 속성에 저장

      - 편향 또는 절편 파라미터 b 는 intercept_ 속성에 저장


   -> 리지 회귀 

      - 회귀를 위한 선형 모델로 가중치의 절댓값을 가능한 작게 만드는 것

      - w의 모든 원소가 0에 가깝게 되길 원함, 직관적으로 생각하면 이는 모든 특성이 출력에 주는 영향을 최소한으로 만듬(기울기)

      - 이런 제약을 '규제' 라고 하며 과대적합이 되지 않도록 모델을 강제로 제한한다는 의미

      - 리지 회귀에 사용하는 규제 방식은 L2 규제라고 함

      - alpha 값의 기본값은 1.0 이며 값을 높이면 0에 더 가깝게 만들어 훈련 세트에 성능은 나빠지지만 일반화에 도움

      - alpha 값을 0.00001로 지정하면 선형 회귀와 같은 score를 보임

      ※ 데이터셋 크기가 400미만에서는 선형 회귀는 어떤 것도 학습을 못하고 있음
          
         두 모델의 성능은 데이터가 많아질수록 좋아지고 마지막엔 선형 회귀가 리지 회귀를 따라잡음

         따라서 데이터가 많으면 리지 회귀와 선형 회귀의 성능이 같아질 것이고 모델이 데이터를 기억하거나 과대적합이 어려워짐

   -> 라소 회귀
       
      - 리지 회귀에서와 같이 라소도 계수를 0에 가깝게 만들려고함, 하지만 방식이 조금 다르고 L1 규제라고함

      - L1 규제의 결과로 라소를 사용할 때 어떤 계수는 정말 0이 됨 = 모델에서 완전히 제외되는 특성이 생김

      - alpha 값과 max_iter(반복 실행하는 최대 횟수)를 활용해 조절

      - 라소 보다는 리지 회귀 모델을 선호 하며 특성이 많고 그 중 일부분만 필요하다면 라소가 좋은 선택일 수 있음

      ※ 라소와 리지의 패널치를 결합한 ElasticNet을 제공, 최상의 성능을 내지만 L1, L2규제를 위한 매개변수 두개를 조정해야 함
       

-----------------------------------------------------------------------------------------------------------------------------------

- 결정 트리 : 분류와 회귀 문제에 널리 사용하는 모델, 기본적으로 결정트리는 결정에 다다르기 위해 예/아니오 질문으로 학습
	   
   -> 결정 트리를 학습한다는 것은 정답에 가장 빨리 도달하는 예/아니오 질문 목록을 학습한다는 뜻   

   -> 머신 러닝에서 이런 질문 들을 테스트라고 함 

   -> 트리 노드 : 질문이나 정답을 담은 네모 상자(마지막 노드는 리프라고도 함)

   -> 엣지 : 질문의 답과 다음 질문을 연결 

   - 1. 트리를 만들 때 알고리즘은 가능한 모든 테스트에서 타깃 값에 대해 가장 많은 정보를 가진 것을 고름

   - 2. 즉, 타깃 값을 가장 잘 나누는 직선을 그리며 루트 노드(전체 데이터 포함)에서 x[1] < 0.06의 테스트에서 분기가 일어남 

   - 3. 테스트를 통과하면 왼쪽 노드에 할당, 못하면 오른쪽 노드에 할당, 가장 좋은 테스트를 찾는 과정을 반복해 모델 생성

   - 4-1. 반복된 프로세스는 각 노드가 테스트 하나씩을 가진 이진 결정 트리를 만듦

   - 4-2. 하나의 축을 따라 데이터를 둘로 나누는 것으로 생각해도 됨
  
   ※ 계층적 영역 분할 알고리즘

   - 5. 각 테스트는 하나의 특성에 대해서만 이루어지므로 나누어진 영역은 항상 축에 평행

   - 6-1. 데이터를 분할하는 것은 각 분할된 영역이(결정 트리의 리프) 

   - 6-2. 한 개의 타깃 값(하나의 클래스나 하나의 회귀분석)을 가질 때 까지 반복 

   - 7. 순수 노드(pure node) : 타깃 하나로만 이뤄진 노드를 이야기 함

   - 8. 새로운 데이터 포인트에 대한 예측은 주어진 데이터 포인트가 특성을 분할한 영역들 중 어디에 놓이는 지 확인하면 됨

   - 9. 영역의 타깃 값 중 다수(순수 노드라면 하나)인 것을 예측 결과로 함

   ※ 회귀의 경우 각 노드의 테스트 결과에 따라 트리를 탐색해나가고 새로운 데이터 포인트에 해당되는 리프노드를 찾음

      찾은 리프노드의 훈련 데이터 평균 값이 이 데이터 포인트의 출력이 됨

   
   -> 결정 트리 복잡도 제어하기 : 모든 리프 노드가 순수 노드가 될 때까지 진행하면 모델이 매우 복잡, 훈련 데이터에 과대적합
 
   - 1-1. 순수 노드로 이루어진 트리 = 훈련 세트에 100% 정확하게 맞음

   - 1-2. 즉 훈련 세트의 모든 데이터 포인트는 정확한 클래스의 리프 노드에 있음
 
   - 1-3. 결정 경계가 클래스의 포인트들에서 멀리 떨어진 이상치(outlier) 하나에 너무 민감함

   - 2-1. 과대 적합을 막는 전략으로는 트리 생성을 일찍 중단하는 전략(사전 가지치기)

   - 2-2. 트리를 만든 후 데이터 포인트가 적은 노드를 삭제하거나 병합하는 전략(사후 가지치기)

   - 3. 사전 가지치기 방법 : 트리의 최대 깊이나 리프의 최대 개수를 제한 또는 노드가 분할하기 위한 포인트 최소 개수 지정

   - 4-1. max_depth 최대 깊이를 정해 연속된 질문을 제한함 = 일정 깊이에 도달하면 트리의 성장을 멈추게 함

   - 4-2. min_sample_split 노드가 분할 하기 위한 포인트 최소 개수 지정 =10 이면 한 노드에 10개의 데이터가 있다면 분기 X 

   - 4-3. max_leaf_nodes  리프의 최대 개수를 제한    	  
   
   -> 특성 중요도 : 트리를 만드는 결정에 각 특성이 얼마나 중요한지 평가, 상위 노드일수록 중요함

   - 1. 특성 중요도 값은 선형 모델 계수와는 달리 항상 양수 이며 특성이 어떤 클래스를 지지하는지 알 수 없음

   - 2-1. 낮은 중요도라고 해서 유용하지 않다고 할 수 없음

   - 2-2 단지 트리가 특성을 선택하지 않았을 뿐 다른 특성이 동일 정보를 지니고 있어서 일 수도 있음

   -> 외삽 : 회귀 문제에서 트리 기반 모델은 훈련 데이터의 범위 밖의 포인트에 대해 예측을 할 수 없음
               
   - 1. 전제 자체가 과거의 데이터에서 미래를 예측하는 것

   - 2. 트리 모델은 훈련 데이터 밖의 새로운 데이터를 예측할 능력이 없음 = 시계열 데이터와는 맞지 않음

   -> 매개 변수 : 사전 가지치기, 위에 언급한 세 가지 중 하나만 지정해도 과대 적합을 막는데 충분함

   -> 장점 : 쉽게 시각화할 수 있어 비전문가 이해 용이, 데이터 스케일에 영향을 받지 않아 특성의 정규화, 표준화 전처리 필요 X 

   -> 단점 : 사전 가지치기를 사용함에도 불구하고 과대적합되는 경향이 있어 일반화 성능이 좋지 못함,

                단일 결정 트리 대안으로 앙상블 방법을 많이 이용함

-----------------------------------------------------------------------------------------------------------------------------------

- 앙상블(ensemble) : 여러 머신러닝 모델 연결해 더 강력한 모델 생성, 두 앙상블 모델이 분류/회귀 다양한 데이터셋의 효과 입증
		랜덤 포레스트(random forest)와 그래이디언 부스팅(gradient boosting) 기본 요소로 결정트리 사용

   -> 랜덤 포레스트(random forest)

   - 1. 기본적으로 조금씩 다른 여러 결정 트리의 묶음

   - 2. 잘 작동하면서, 서로 다른 방향으로 과대적합된 트리 생성 많이하고, 결과 평균을 내어 과대적합을 감소 시키는게 핵심

   - 3. 타깃 예측을 잘 해야하고 다른 트리와 구별 되어야하는 결정 트리를 많이 많들어야 함

   - 4. 트리를 랜덤으로 만드는 방법으로는 무작위성 주입을 기초로 함

   - 5-1. 데이터 포인트를 무작위로 선택하는 방법

   - 5-2. 분할 테스트에서 특성을 무작위로 선택하는 방법

   - 6. 모델을 구축하기 위해선 생성할 트리의 개수를 정해야 함(n_estimators 기본 값이 100)

   - 7-1. 독립적이고 고유한 트리를 만들기 위해 데이터의 부트스트랩 샘플(bootstrap sample)을 생성 

   - 7-2. 다시 말해  n_samples개의 데이터 포인트 중에서 무작위로 데이터 n_samples 횟수 만큼 반복 추출(중복 추출 가능)

   - 7-3. 이 데이터 셋은 원래 데이터셋 크리와 같지만 어떤 데이터 포인트는 누락될 수도 있고(1/3) 어떤 것은 중복될 수 있음

   - 8-1. 이렇게 만든 데이터 셋으로 결정트리를 만듦, 각 노드에서 전체 특성을 대상으로 최선의 테스트를 찾는 것이 아님

   - 8-2. 알고리즘이 각 노드에서 후보 특성을 무작위로 선택한 후 이 후보들 중에서 최선의 테스트를 찾음

   - 8-3. 몇 개의 특성을 고를지는 max_features 매개변수로 조정할 수 있음

   - 8-4. 후보 특성을 고르는 것은 매 노드마다 반복되므로 투리의 각 노드는 다른 후보 특성들을 사용해 테스트를 만듦

   - 9. 부트스트랩 샘플링은 랜덤 포레스트의 트리가 조금씩 다른 데이터셋을 이용해 만들어지도록 함 

   - 10. 각 노드에서 특성의 일부만 사용하기 때문에 트리의 각 분기는 각기 다른 특성 부분 집합을 사용

   - 11. 이 두 매커니즘이 합쳐져 랜덤 포레스트의 모든 트리가 달라지게 끔 함

   - 12. 핵심 매개변수는 max_features이며 n_features에 가까울 수록 무작위성이 떨어짐(트리들이 서로 비슷해짐)

   - 13. RF로 예측 할 때는 먼저 알고리즘이 모델에 있는 모든 트리의 예측을 만듦

   - 14. 회귀의 경우 이 예측들을 평균하여 최종 예측을 만들고 분류는 약한 투표 전략을 사용

   - 15. 즉 각 알고리즘이 가능성 있는 출력 레이블의 확률을 제공함으로써 간접적 예측을 진행 

   - 16. 트리들이 예측한 확률을 평균내어 가장 높은 확률을 가진 클래스가 예측값이 됨
   
   -> 특성 중요도 : RF를 만드는 무작위성은 알고리즘이 가능성 있는 많은 경우를 고려하므로 더 넓은 시각으로 볼 수 있음

   -> 매개 변수 : n_jobs 매개 변수 이용해 CPU 코어 개수에 비례해 처리 속도 높임(-1 전부 사용)
                     
	        RF 트리가 많을 수록(n_estimators) random_state 값의 변화에 따른 변동이 적음

 	        max_features 일반적으로 기본값을 쓰는 것이 가장 좋은 방법('auto', √n)

                     분류는 max_features = sqrt(n_features) 회귀는 max_features = n_features

   -> 장점 : 매개 변수 튜닝이 많이 없이도 잘 작동, 스케일 조정 필요X, 트리는 많을 수록 좋지만 시간과 메모리가 이어짐 

   -> 단점 : 텍스트 데이터 같이 매우 차원이 높고 희소한 데이터에는 잘 작동하지 않음(선형 모델 적합)
             
               선형 모델보다 많은 메모리를 사용하고 훈련과 예측이 느림


  